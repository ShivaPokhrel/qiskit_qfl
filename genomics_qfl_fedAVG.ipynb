{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum Federated Learning with Genomic Data\n",
        "\n",
        "This Jupyter Notebook demonstrates the use of Quantum Federated Learning (QFL) with genomic data. Quantum computing has the potential to revolutionize machine learning by offering unique computational advantages. In this notebook, we'll use the Qiskit and Genomic Benchmarks libraries to explore the concept of federated learning, which is a decentralized approach to machine learning.\n",
        "\n",
        "## Required Dependencies\n",
        "\n",
        "Before proceeding with the code execution, it is essential to ensure that you have the necessary libraries installed. The following commands will help you install these libraries:\n",
        "\n",
        "The line `%%capture` prevents any pip logs from being displayed here.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9g8JHvv0enTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2c5GCDkkqIB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection\n",
        "\n",
        "In this section, our main objective is to gather the necessary data for our Quantum Federated Learning experiment. We will use the `genomic_benchmarks` library to work with a dataset designed for classifying DNA sequences as either human or worm.\n",
        "\n",
        "To start collecting our data, we'll import the required dataset using the `DemoHumanOrWorm` class from the `genomic_benchmarks.dataset_getters.pytorch_datasets` module. This dataset comes with both a training set and a test set. However, during testing, we noticed some issues with the `test_set` variable in fetching the correct data. For our current purpose, we'll focus solely on the `train_set` variable, which holds a substantial 75,000 samples.\n",
        "\n",
        "> If your specific use case requires it, you can include the test set as well by uncommenting the relevant line of code.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_-qZP0pxkh9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplzhnMjkt-h"
      },
      "outputs": [],
      "source": [
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Set Size\n",
        "\n",
        "Before we move further, let's check the size of the testing and training set variables. We have previously mentioned that there were some issues with the `test_set` variable during data collection. We'll assess its current state."
      ],
      "metadata": {
        "id": "n0SQqXpClgYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7Fx2AmWY9S5"
      },
      "outputs": [],
      "source": [
        "print(f\"Nuber of samples in the test set: {len(test_set)}\")\n",
        "print(f\"Nuber of samples in the test set: {len(train_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genomics Data\n",
        "\n",
        "Now, let's take a closer look at what the genomic data looks like. The data consists of DNA sequences, each represented as a string with a length of 200 characters, and an associated label, which can be either 0 or 1. In this context, 0 typically represents human DNA, while 1 corresponds to worm DNA.\n",
        "\n",
        "For our specific use case, we need to reduce the dimensionality of this data. One approach is to encode the DNA sequence characters as follows:\n",
        "- 'A' as 1\n",
        "- 'T' as 2\n",
        "- 'C' as 3\n",
        "- 'G' as 4\n",
        "- 'N' as 5\n",
        "\n",
        "Since we know that DNA sequences contain only these characters. However, working with 200 features for each sequence might be too complex. In the next step, we'll work on reducing the dimensionality of this data from 200 features down to a single digit, such as 5 or 4.\n"
      ],
      "metadata": {
        "id": "8V9VonkjmA-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX8F6CCxlOFC"
      },
      "outputs": [],
      "source": [
        "print(\"One sample from the data_set variable: \")\n",
        "data_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dictionary of DNA Sequence Representations\n",
        "\n",
        "In the following code snippet, we create a dictionary named `word_combinations`. This dictionary is designed to hold numerical representations of DNA sequences. Our goal is to convert the DNA sequences into a more manageable format.\n",
        "\n",
        "To achieve this, we define a variable called `word_size`, which specifies the length of each word we want to consider in the DNA sequence. In this particular case, `word_size` is set to 40, but you can adjust it based on your specific requirements.\n",
        "\n",
        "The code iterates through each DNA sequence in the `data_set` and extracts overlapping subsequences of length `word_size`. These subsequences are represented as \"words.\" For each unique word encountered in the DNA sequences, we assign a numerical representation.\n",
        "\n",
        "The resulting `word_combinations` dictionary stores these numerical representations, allowing us to work with a simplified version of the DNA data.\n"
      ],
      "metadata": {
        "id": "4dgnS0vGnrfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyRLgiQ0AeJI"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "word_size = 40\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at how the word combination dict actually looks like"
      ],
      "metadata": {
        "id": "yh2iTcUxgyxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining Word Combinations and Data Sample\n",
        "\n",
        "Let's explore the contents of our `word_combinations` variable and relate it to the first sample from our data.\n",
        "\n",
        "We can observe that the sample consists of a DNA sequence of 200 characters, with the label 0. The word_combinations dictionary assigns numerical labels to DNA sequence subsequences. For example, the first 40 characters of the sample are labeled as 1, the next 40 characters are labeled as 2, and so on.\n",
        "\n",
        "The first sample in the `data_set` variable is as follows:\n",
        "```python\n",
        "('GTGTCATTCAGCCAAGAGGCAAAAATAAACCACAGTTTTTTTTCCATTCTTTTAAGCCAATAAGCAGTACATGGTATGATTATTGGGGTGCTGATTATGGGACCTACAATTACAACCCTTACATTGGAGGTCTGGGAATTCCTGTAGCAAAGCCACCAGCAAACACGGAGAAGAACGGATCACAGACAGTTAGCGTTTCA', 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "EYLTSrm-pEO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)"
      ],
      "metadata": {
        "id": "W1yrmwTmYHm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding DNA Sequences\n",
        "\n",
        "In the following code segment, we encode the 200-character DNA sequences into smaller samples, each comprising 200 divided by the `word_size` segments. This encoding allows us to represent the DNA sequences in a numerical format by assigning a numerical value from the `word_combinations` dictionary to each segment.\n",
        "\n",
        "The specific steps in the code include:\n",
        "1. Stripping any leading or trailing whitespace from the DNA sequence.\n",
        "2. Dividing the DNA sequence into `word_size`-letter word segments using a sliding window approach.\n",
        "3. Converting these word segments into their corresponding numerical values as per the `word_combinations` dictionary.\n",
        "4. Organizing the data into a structured format that includes the numerical sequence and its associated label.\n",
        "\n",
        "The resulting `np_data_set` holds these encoded data points."
      ],
      "metadata": {
        "id": "L73Bf8hWqcfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyT-50_ElB-j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PePG3-CYqtO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffling Data for Balanced Distribution\n",
        "\n",
        "In the code segment above, we observe the first 5 samples of the `np_data_set` variable. It's apparent that all of these initial samples have a label of 0. This observation is due to the common dataset structure, where the data is organized such that the first batch of samples belongs to one class (in this case, class 0), followed by another class (class 1), and so on.\n",
        "\n",
        "However, in the subsequent steps of this code, we'll divide the data into portions for each of our clients, and it's crucial to ensure that each client receives a balanced mix of data from both classes (0 and 1). Therefore, we need to shuffle the `np_data_set` variable.\n",
        "\n",
        "Shuffling the dataset randomizes the order of samples, guaranteeing that no single client will receive data only from one class. This is essential for a more representative and fair distribution of data among clients.\n"
      ],
      "metadata": {
        "id": "sqGkiiy1rYXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc2GEgyDm06m"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(np_data_set)\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling the Data with Min-Max Scaling\n",
        "\n",
        "In the code provided, we apply Min-Max scaling to the dataset to normalize the numerical values. This process is valuable for ensuring that the features of the dataset are within a consistent range, typically between 0 and 1.\n",
        "\n",
        "Here's how the code accomplishes this:\n",
        "1. We collect the sequences from the `np_data_set` variable and stack them into an array.\n",
        "2. We create a `MinMaxScaler` object, which will be used to perform the scaling.\n",
        "3. The scaler is then applied to the sequences using `scaler.fit_transform()`.\n",
        "4. The scaled sequences are replaced in each data point within the `np_data_set` variable.\n",
        "\n",
        "As a result, the sequences' values are transformed to a standardized scale, making it easier to work with the data and ensuring that each feature has the same weight in subsequent analyses.\n",
        "\n",
        "The output displays the first 5 samples of the scaled, encoded, and shuffled data, highlighting how the values are now in the [0, 1] range after the scaling process.\n"
      ],
      "metadata": {
        "id": "KYHANG9ir_Dx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWzmu8iCGew2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Dataset and Preparing Test Data\n",
        "\n",
        "In the previous section, we divided the `np_data_set` variable into two subsets, with 70,000 samples earmarked for training and 5,000 samples reserved for testing. This division is crucial for the development and evaluation of our Quantum Federated Learning model, ensuring that we have separate datasets for these purposes.\n",
        "\n",
        "Following the split, we proceed to prepare the test data for further analysis and evaluation. We extract the sequences and labels from the testing dataset. This separation is essential as it allows us to analyze the data and labels separately, facilitating model evaluation and performance assessment.\n",
        "\n",
        "At this point, the test data is organized into two variables:\n",
        "- `test_sequences`: An array containing the sequences from the test data.\n",
        "- `test_labels`: An array containing the corresponding labels from the test data.\n",
        "\n",
        "These variables will be used in subsequent steps to evaluate the model's performance on the testing data.\n"
      ],
      "metadata": {
        "id": "WDm9dobfsqMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxRp2adYOM88"
      },
      "outputs": [],
      "source": [
        "np_train_data = np_data_set[:70000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the Federated Learning Setup\n",
        "\n",
        "In this code section, we establish essential variables and settings for our Federated Learning setup. These variables play a crucial role in shaping how the Federated Learning process unfolds and offer the flexibility to customize the experiment to meet specific requirements.\n",
        "\n",
        "Here, we outline the key variables that we define:\n",
        "\n",
        "- `num_clients`: This variable determines the number of participating clients in our Federated Learning setup. Each client plays a role in the learning process.\n",
        "- `num_epochs`: It specifies the number of training epochs, indicating how many times the Federated Learning process will iterate through the training data for each client.\n",
        "- `max_train_iterations`: This variable controls the maximum number of training iterations that each client will perform during each round of Federated Learning.\n",
        "- `samples_per_epoch`: It defines the number of samples processed in each training epoch for each client.\n",
        "- `backend`: The choice of backend, specified as 'aer_simulator' in this code, determines the quantum simulator used for the Federated Learning setup. If you intend to work with a real quantum device, you can replace this backend with a real quantum device backend provided by IBM Quantum.\n",
        "\n",
        "It's important to note that you can adjust these values as needed, depending on your specific use case and testing requirements. However, there is a critical constraint to consider: Ensure that the size of the `np_train_data` dataset is less than or equal to the product of `num_clients`, `num_epochs`, and `samples_per_epoch`. This constraint ensures that there is sufficient data to split between each client.\n",
        "\n",
        "If you wish to work with a real quantum backend, the following code snippet shows how to load the IBM Quantum account and set the appropriate backend:\n",
        "```python\n",
        "from qiskit import Aer, IBMQ\n",
        "\n",
        "# Load your IBM Quantum account\n",
        "IBMQ.load_account()\n",
        "\n",
        "# Access the provider with the desired backend\n",
        "provider = IBMQ.get_provider(hub='ibm-q', group='open', project='main')\n",
        "\n",
        "# List available backends\n",
        "provider.backends()\n",
        "\n",
        "# Define the backend for your quantum computations\n",
        "backend = provider.get_backend('ibm_nairobi')\n"
      ],
      "metadata": {
        "id": "CuXuQDNZ-7Xu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDSWWIXLz8sP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit import Aer, IBMQ\n",
        "\n",
        "\n",
        "num_clients = 5\n",
        "num_epochs = 100\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=100\n",
        "backend = Aer.get_backend('aer_simulator')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Client Class and Splitting the Training Data\n",
        "\n",
        "In the code below, we take two significant actions: defining a `Client` class and splitting the training data into multiple clients.\n",
        "\n",
        "**Client Class**:\n",
        "- The `Client` class is introduced to encapsulate essential information for each client in our Federated Learning setup. Each client has attributes like `models`, `primary_model`, `data`, `test_scores`, and `train_scores`. These attributes are crucial for managing and tracking the client's involvement in the Federated Learning process.\n",
        "\n",
        "**Data Splitting Function**:\n",
        "- A function named `split_dataset` is defined to split the training data into segments, with each segment designated for a specific client and epoch. The function takes parameters such as `num_clients`, `num_epochs`, and `samples_per_epoch` to control the data splitting process.\n",
        "\n",
        "**Creating Client Instances**:\n",
        "- After defining the class and data splitting function, we proceed to create an array called `clients`. This array is populated with instances of the `Client` class, and each instance contains the relevant data segments for each epoch. This division ensures that each client has access to its designated training data.\n",
        "\n",
        "This code sets the foundation for managing clients and their data within the Federated Learning framework.\n"
      ],
      "metadata": {
        "id": "OlwGSN2MDiIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FSllMLQ59uT"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "    def __init__(self, data):\n",
        "        self.models = []\n",
        "        self.primary_model = None\n",
        "        self.data = data\n",
        "        self.test_scores = []\n",
        "        self.train_scores = []\n",
        "\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "  clients = []\n",
        "  for i in range(num_clients):\n",
        "    client_data = []\n",
        "    for j in range(num_epochs):\n",
        "      start_idx = (i*num_epochs*samples_per_epoch)+(j*samples_per_epoch)\n",
        "      end_idx = (i*num_epochs*samples_per_epoch)+((j+1)*samples_per_epoch)\n",
        "      client_data.append(np_train_data[start_idx:end_idx])\n",
        "    clients.append(Client(client_data))\n",
        "  return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining Client Data\n",
        "\n",
        "The code snippet `clients[0].data[0][:3]` is used to display the data for the first client and its first epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "gpPR91ueFmBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7V9ZtNF8LZH"
      },
      "outputs": [],
      "source": [
        "clients[0].data[0][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function for Federated Learning\n",
        "\n",
        "In the following code, we define a fundamental function used during the Federated Learning phase for each client in each epoch. This function, named `train`, takes the client's data for a specific epoch and trains a model accordingly. It returns the trained model, training score, as well as the testing score for that iteration.\n",
        "\n",
        "**Function Explanation**:\n",
        "- The `train` function begins by checking if a model has been provided as an argument. If not (this is only for the first epoch that a client does not have thier model), it initializes a model for training. This model is created using the Quantum Variational Circuit (QVC) framework and includes components like the feature map, ansatz, optimizer, and a callback function for tracking the training progress.\n",
        "\n",
        "- The function then processes the training data by extracting the sequences and labels from the provided data. These sequences and labels are organized into NumPy arrays for compatibility with the model.\n",
        "\n",
        "- The training process is initiated, and the function measures the time taken for training. Upon completion, it prints the time elapsed during training.\n",
        "\n",
        "- The model's performance is evaluated by scoring it on both the training and `200` samples in the testing data. The training score and testing score are computed and returned.\n",
        "\n",
        "**Callback Function**:\n",
        "- A callback function named `training_callback` is used to monitor the training progress. This function is called by the VQC class and receives weights and the corresponding objective function evaluation during the optimization process. Currently, the function simply tracks the iteration value, but we can extend its functionality as needed.\n",
        "\n",
        "This `train` function plays a central role in the Federated Learning process, enabling clients to train models and evaluate their performance for each epoch.\n"
      ],
      "metadata": {
        "id": "UoC-RJwsEzvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku0bQ1KloCAa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "itr = 0\n",
        "def training_callback(weights, obj_func_eval):\n",
        "        global itr\n",
        "        itr += 1\n",
        "        print(f\"{itr}\", end=' | ')\n",
        "\n",
        "def train(data, model = None):\n",
        "  if model is None:\n",
        "    num_features = len(data[0][\"sequence\"])\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "    vqc_model = VQC(\n",
        "        feature_map=feature_map,\n",
        "        ansatz=ansatz,\n",
        "        optimizer=optimizer,\n",
        "        callback=partial(training_callback),\n",
        "        sampler=BackendSampler(backend=backend),\n",
        "        warm_start=True\n",
        "    )\n",
        "    model = vqc_model\n",
        "\n",
        "  train_sequences = [data_point[\"sequence\"] for data_point in data]\n",
        "  train_labels = [data_point[\"label\"] for data_point in data]\n",
        "\n",
        "  # Convert the lists to NumPy arrays\n",
        "  train_sequences = np.array(train_sequences)\n",
        "  train_labels = np.array(train_labels)\n",
        "\n",
        "  # Print the shapes\n",
        "  print(\"Train Sequences Shape:\", train_sequences.shape)\n",
        "  print(\"Train Labels Shape:\", train_labels.shape)\n",
        "\n",
        "  print(\"Training Started\")\n",
        "  start_time = time.time()\n",
        "  model.fit(train_sequences, train_labels)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nTraining complete. Time taken: {elapsed_time} seconds.\")\n",
        "\n",
        "  print(f\"SCORING MODEL\")\n",
        "  train_score_q = model.score(train_sequences, train_labels)\n",
        "  test_score_q = model.score(test_sequences[:200], test_labels[:200])\n",
        "  return train_score_q, test_score_q, model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Accuracy and Creation Functions\n",
        "\n",
        "In the provided code, two essential functions are defined, each with a specific role.\n",
        "\n",
        "**`getAccuracy` Function**:\n",
        "- The `getAccuracy` function calculates and returns the accuracy of a model with given weights. It initializes a Quantum Variational Circuit (QVC) model with the provided weights and prepares it for evaluation. While it includes a call to the training function (`vqc.fit()`), the training itself doesn't occur because we set the maximum iteration value of the optimizer to 0. This is done as a workaround because we cannot directly use the `.score` function without first executing the `.fit` function on a new VQC class instance. After model preparation, the function computes the accuracy by evaluating the model's performance using test sequences and labels.\n",
        "\n",
        "**`create_model_with_weights` Function**:\n",
        "- The `create_model_with_weights` function creates a new Quantum Variational Circuit (QVC) model with an initial point set to the given weights. This function is instrumental in creating a global model from global model weights during the Federated Learning training process.\n"
      ],
      "metadata": {
        "id": "pyCXpYKfGkHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAomm8pqVsh8"
      },
      "outputs": [],
      "source": [
        "def getAccuracy(weights):\n",
        "        num_features = len(test_sequences[0])\n",
        "        feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "        ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "        ansatz = ansatz.bind_parameters(weights)\n",
        "        optimizer = COBYLA(maxiter=0)\n",
        "        vqc = VQC(\n",
        "            feature_map=feature_map,\n",
        "            ansatz=ansatz,\n",
        "            optimizer=optimizer,\n",
        "            sampler=BackendSampler(backend=backend)\n",
        "        )\n",
        "        vqc.fit(test_sequences[:25], test_labels[:25])\n",
        "        return vqc.score(test_sequences[:200], test_labels[:200])\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "  num_features = len(test_sequences[0])\n",
        "  feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "  ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "  optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "  vqc = VQC(\n",
        "      feature_map=feature_map,\n",
        "      ansatz=ansatz,\n",
        "      optimizer=optimizer,\n",
        "      sampler=BackendSampler(backend=backend),\n",
        "      warm_start = True,\n",
        "      initial_point  = weights,\n",
        "      callback=partial(training_callback)\n",
        "  )\n",
        "  return vqc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEgSu9OAJTDQ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Temporary code to suppress all FutureWarnings for a cleaner output\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wwqgtmCzL21F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Learning Training Loop\n",
        "\n",
        "In this code, we implement the training loop for Federated Learning across multiple epochs and clients. The key steps and processes are as follows:\n",
        "\n",
        "1. **Epoch-by-Epoch Training**: The code iterates through each epoch, starting with epoch 0, and prepares to train client models for that epoch.\n",
        "\n",
        "2. **Client Training**: For each epoch, the code goes through all the clients, one by one. It checks if the client has a primary model. If not, it creates a new model and trains the model using the training data specific to that client and epoch. The resulting model is stored in the client's `models` array. The training process also calculates and stores the training and testing scores for each client.\n",
        "\n",
        "3. **Global Model Aggregation**: After training the models for all clients in a given epoch, the code collects the trained model weights in an array called `epoch_weights`. This array holds the weights of all client models and the global model from the previous epoch (if applicable).\n",
        "\n",
        "4. **Global Model Averaging**: The code computes the average of the weights in the `epoch_weights` array to create a global model for that epoch. This global model represents a consensus model generated from the weighted combination of client models and the previous global model (if any).\n",
        " - To extend the\n",
        "\n",
        "5. **Client Model Update**: The new global model is assigned to each client as its primary model for the next epoch. This ensures that all clients train using the same global model in the following epoch.\n",
        "\n",
        "6. **Global Model Evaluation**: The code evaluates the accuracy of the global model on a subset of testing data. The accuracy is stored in the `global_model_accuracy` array for tracking the performance of the global model over different epochs.\n",
        "\n",
        "The code proceeds to repeat this process for all specified epochs. This iterative training and global model update are essential for Federated Learning, allowing clients to contribute to the global model while maintaining their own local data privacy and model customization.\n"
      ],
      "metadata": {
        "id": "bniOspxSMKHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZADDG1kj8g8i"
      },
      "outputs": [],
      "source": [
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  global_model_weights[epoch] = []\n",
        "  epoch_weights = []\n",
        "  print(f\"epoch: {epoch}\")\n",
        "\n",
        "  for index, client in enumerate(clients):\n",
        "    print(f\"Index: {index}, Client: {client}\")\n",
        "\n",
        "    if client.primary_model is None:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch])\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      # Print the values\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "    else:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch], model = client.primary_model)\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "  if(epoch != 0):\n",
        "    epoch_weights.append(global_model_weights[epoch-1])\n",
        "\n",
        "  average_weights = sum(epoch_weights) / len(epoch_weights)\n",
        "\n",
        "  global_model_weights[epoch] = average_weights\n",
        "  new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "  for index, client in enumerate(clients):\n",
        "    client.primary_model = new_model_with_global_weights\n",
        "\n",
        "  global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "  print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy}\")\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Client Training Scores\n",
        "\n",
        "In this code, we create visualizations to track the training and testing scores of each client across different epochs.\n"
      ],
      "metadata": {
        "id": "zncGRl8rNLAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8zqIGtNOcN1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create two figures, one for train scores and one for test scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot train scores for all clients\n",
        "for client in clients:\n",
        "    plt.plot(client.train_scores, label=f'Client {clients.index(client) + 1}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Train Score')\n",
        "plt.title('Train Scores for All Clients')\n",
        "plt.legend()\n",
        "\n",
        "# Show the train scores plot\n",
        "plt.show()\n",
        "\n",
        "# Create a new figure for test scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot test scores for all clients\n",
        "for client in clients:\n",
        "    plt.plot(client.test_scores, label=f'Client {clients.index(client) + 1}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test Score')\n",
        "plt.title('Test Scores for All Clients')\n",
        "plt.legend()\n",
        "\n",
        "# Show the test scores plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Client Training Scores\n",
        "\n",
        "In this code, we create visualizations to testing scores of each client and the global model across different epochs.\n"
      ],
      "metadata": {
        "id": "_-mWDgQZNZHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BENho8qvOcpC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure for the test scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot test scores for all clients\n",
        "for client in clients:\n",
        "    plt.plot(client.test_scores, label=f'Client {clients.index(client) + 1}')\n",
        "\n",
        "# Plot global model accuracy\n",
        "plt.plot(global_model_accuracy, label='Global Model Accuracy', linestyle='--', color='black')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Test Scores and Global Model Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show the combined graph\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}